{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    RocCurveDisplay,\n",
    "    PrecisionRecallDisplay,\n",
    "    precision_recall_curve,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.display.float_format = \"{:,.3f}\".format\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned training data\n",
    "train_path = Path(\"../data/Train_Cleaned.csv\")\n",
    "df = pd.read_csv(train_path)\n",
    "\n",
    "# Encode target\n",
    "raw_target = df[\"PotentialFraud\"]\n",
    "encoded_target = pd.to_numeric(raw_target.replace({\"Yes\": 1, \"No\": 0}), errors=\"coerce\")\n",
    "if encoded_target.isna().any():\n",
    "    raise ValueError(\"Unexpected labels in PotentialFraud; extend mapping to proceed.\")\n",
    "y = encoded_target.astype(int)\n",
    "\n",
    "# Features exclude label and provider identifier (if present)\n",
    "feature_cols = [c for c in df.columns if c not in (\"PotentialFraud\", \"Provider\")]\n",
    "X = df[feature_cols]\n",
    "\n",
    "# Stratified 60/20/20 split\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "print(\"Label distribution (positive rate):\")\n",
    "for name, series in {\"train\": y_train, \"val\": y_val, \"test\": y_test}.items():\n",
    "    print(f\"  {name:<5}: {series.mean():.3f} ({series.sum()} / {len(series)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric helpers\n",
    "\n",
    "def summarize_metrics(y_true, preds, probs):\n",
    "    return {\n",
    "        \"precision\": precision_score(y_true, preds, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, preds, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, preds, zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(y_true, probs),\n",
    "        \"pr_auc\": average_precision_score(y_true, probs)\n",
    "    }\n",
    "\n",
    "\n",
    "def metric_row(split, y_true, preds, probs):\n",
    "    return {\"split\": split, **summarize_metrics(y_true, preds, probs)}\n",
    "\n",
    "\n",
    "def sweep_thresholds(y_true, probs, thresholds, fp_cost=1_000, fn_cost=10_000):\n",
    "    rows = []\n",
    "    for t in thresholds:\n",
    "        preds = (probs >= t).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, preds, labels=[0, 1]).ravel()\n",
    "        metrics = summarize_metrics(y_true, preds, probs)\n",
    "        rows.append({\n",
    "            \"threshold\": round(float(t), 4),\n",
    "            **metrics,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"expected_cost\": fp * fp_cost + fn * fn_cost\n",
    "        })\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold stratified CV on the dev set (train + val)\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "dev_X = pd.concat([X_train, X_val])\n",
    "dev_y = pd.concat([y_train, y_val])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_probs = cross_val_predict(\n",
    "    gb_model,\n",
    "    dev_X,\n",
    "    dev_y,\n",
    "    cv=skf,\n",
    "    method=\"predict_proba\",\n",
    "    n_jobs=-1\n",
    ")[:, 1]\n",
    "cv_preds = (cv_probs >= 0.5).astype(int)\n",
    "\n",
    "cv_metrics = metric_row(\"cv_5fold\", dev_y, cv_preds, cv_probs)\n",
    "pd.DataFrame([cv_metrics]).set_index(\"split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on training set and tune threshold on validation\n",
    "val_model = GradientBoostingClassifier(random_state=42)\n",
    "val_model.fit(X_train, y_train)\n",
    "val_probs = val_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "prec, rec, thresholds = precision_recall_curve(y_val, val_probs)\n",
    "f1_scores = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "best_idx = int(np.argmax(f1_scores[:-1]))  # last precision/recall pair has no threshold\n",
    "optimal_threshold = float(thresholds[best_idx])\n",
    "\n",
    "val_preds = (val_probs >= optimal_threshold).astype(int)\n",
    "val_metrics = metric_row(\"validation\", y_val, val_preds, val_probs)\n",
    "print(f\"Optimal threshold from validation PR curve: {optimal_threshold:.3f}\")\n",
    "\n",
    "# Refit on train+val and evaluate on held-out test\n",
    "gb_model.fit(dev_X, dev_y)\n",
    "test_probs = gb_model.predict_proba(X_test)[:, 1]\n",
    "test_preds = (test_probs >= optimal_threshold).astype(int)\n",
    "test_metrics = metric_row(\"test\", y_test, test_preds, test_probs)\n",
    "\n",
    "metric_table = pd.DataFrame([cv_metrics, val_metrics, test_metrics]).set_index(\"split\")\n",
    "print(\"Precision/Recall/F1/ROC-AUC/PR-AUC summary\")\n",
    "display(metric_table)\n",
    "\n",
    "print(\"Classification report (test):\")\n",
    "print(classification_report(y_test, test_preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ConfusionMatrixDisplay.from_predictions(y_val, val_preds, cmap=\"Blues\", ax=axes[0])\n",
    "axes[0].set_title(\"Validation confusion matrix\")\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, test_preds, cmap=\"Blues\", ax=axes[1])\n",
    "axes[1].set_title(\"Test confusion matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "RocCurveDisplay.from_predictions(y_test, test_probs, name=\"GB\", ax=axes[0])\n",
    "axes[0].set_title(\"ROC curve (test)\")\n",
    "PrecisionRecallDisplay.from_predictions(y_test, test_probs, name=\"GB\", ax=axes[1])\n",
    "axes[1].set_title(\"Precisionâ€“Recall curve (test)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP_COST = 1_000   # adjust to actual investigation cost\n",
    "FN_COST = 10_000  # adjust to expected loss from a missed fraud case\n",
    "\n",
    "threshold_grid = np.linspace(0.05, 0.95, 19)\n",
    "cost_table = sweep_thresholds(y_val, val_probs, threshold_grid, fp_cost=FP_COST, fn_cost=FN_COST)\n",
    "\n",
    "best_cost_row = cost_table.sort_values(\"expected_cost\").iloc[0]\n",
    "print(\"Best cost-weighted threshold:\")\n",
    "display(best_cost_row.to_frame().T)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.lineplot(data=cost_table, x=\"threshold\", y=\"expected_cost\", marker=\"o\")\n",
    "plt.axvline(best_cost_row[\"threshold\"], color=\"red\", linestyle=\"--\", label=\"Min cost threshold\")\n",
    "plt.title(\"Expected cost by threshold (validation)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = X_test.copy()\n",
    "analysis_df[\"y_true\"] = y_test.values\n",
    "analysis_df[\"pred\"] = test_preds\n",
    "analysis_df[\"proba\"] = test_probs\n",
    "analysis_df[\"error_type\"] = np.select(\n",
    "    [\n",
    "        (analysis_df[\"pred\"] == 1) & (analysis_df[\"y_true\"] == 0),\n",
    "        (analysis_df[\"pred\"] == 0) & (analysis_df[\"y_true\"] == 1)\n",
    "    ],\n",
    "    [\"False Positive\", \"False Negative\"],\n",
    "    default=\"Correct\"\n",
    ")\n",
    "\n",
    "ref_mean = X_train.mean()\n",
    "ref_std = X_train.std().replace(0, np.nan)\n",
    "\n",
    "\n",
    "def top_drivers(row, top_n=5):\n",
    "    z = ((row - ref_mean) / ref_std).abs().sort_values(ascending=False).head(top_n)\n",
    "    return \"; \".join([f\"{idx} (z={val:.1f})\" for idx, val in z.items()])\n",
    "\n",
    "fp_cases = analysis_df.loc[analysis_df[\"error_type\"] == \"False Positive\"].copy()\n",
    "fn_cases = analysis_df.loc[analysis_df[\"error_type\"] == \"False Negative\"].copy()\n",
    "\n",
    "fp_cases[\"drivers\"] = fp_cases[feature_cols].apply(top_drivers, axis=1)\n",
    "fn_cases[\"drivers\"] = fn_cases[feature_cols].apply(top_drivers, axis=1)\n",
    "\n",
    "print(\"False positives (top 3 by fraud probability):\")\n",
    "display(fp_cases.sort_values(\"proba\", ascending=False)[[\"proba\", \"drivers\"]].head(3))\n",
    "\n",
    "print(\"False negatives (top 3 by lowest fraud probability among actual frauds):\")\n",
    "display(fn_cases.sort_values(\"proba\", ascending=True)[[\"proba\", \"drivers\"]].head(3))\n",
    "\n",
    "print(\"Error mix on held-out test split:\")\n",
    "display(analysis_df[\"error_type\"].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraudproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
